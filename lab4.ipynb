{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4418ab5",
   "metadata": {},
   "source": [
    "\n",
    "# Laboratorio #3 – CNN con MNIST (Pasos 2 y 3)\n",
    "**Curso:** Inteligencia Artificial  \n",
    "**Tema:** Convolutional Neural Networks (CNN)  \n",
    "**Dataset:** MNIST (28×28, 10 clases)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf18066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, random, time, itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fb3364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10               #\n",
    "LR = 1e-3\n",
    "DATA_DIR = \"./data\"      \n",
    "VAL_SPLIT = 10_000       \n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80026b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Datos: MNIST con normalización estándar\n",
    "# Promedio y desviación típica de MNIST en escala [0,1]:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "try:\n",
    "    full_train = datasets.MNIST(root=DATA_DIR, train=True, transform=transform, download=True)\n",
    "    test_ds = datasets.MNIST(root=DATA_DIR, train=False, transform=transform, download=True)\n",
    "except Exception as e:\n",
    "    print(\" No se pudo descargar/cargar MNIST automáticamente.\")\n",
    "    print(\"   Error:\", e)\n",
    "    print(\"   Solución: Descarga manualmente MNIST y coloca los archivos en\", DATA_DIR)\n",
    "    raise\n",
    "\n",
    "# División train/val\n",
    "train_size = len(full_train) - VAL_SPLIT\n",
    "val_size = VAL_SPLIT\n",
    "train_ds, val_ds = random_split(full_train, [train_size, val_size], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a858d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utilidades: conteo de parámetros, accuracy, bucles de train/eval, matriz de confusión\n",
    "\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def accuracy_from_logits(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    preds = logits.argmax(dim=1)\n",
    "    correct = (preds == y).sum().item()\n",
    "    return correct / y.size(0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_count = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total_count += y.size(0)\n",
    "    return total_loss / total_count, total_correct / total_count\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, running_correct, running_count = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * y.size(0)\n",
    "        running_correct += (logits.argmax(1) == y).sum().item()\n",
    "        running_count += y.size(0)\n",
    "    return running_loss / running_count, running_correct / running_count\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=EPOCHS):\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_val = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        print(f\"Epoch {ep:02d}/{epochs} | Train Loss {tr_loss:.4f} Acc {tr_acc:.4f} | Val Loss {val_loss:.4f} Acc {val_acc:.4f}\")\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return history\n",
    "\n",
    "@torch.no_grad()\n",
    "def confusion_matrix(model: nn.Module, loader: DataLoader, num_classes: int = 10, device: torch.device = torch.device(\"cpu\")) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(1)\n",
    "        for t, p in zip(y.view(-1), preds.view(-1)):\n",
    "            cm[t.long(), p.long()] += 1\n",
    "    return cm.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6477bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modelos \n",
    "class MLPBaseline(nn.Module):\n",
    "    def __init__(self, hidden1: int = 256, hidden2: int = 128, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, hidden1), nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden2), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden2, num_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, c1: int = 16, c2: int = 32, fc_hidden: int = 128, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        # Bloque 1: Conv -> ReLU -> MaxPool\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=c1, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Bloque 2: Conv -> ReLU -> MaxPool\n",
    "        self.conv2 = nn.Conv2d(in_channels=c1, out_channels=c2, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Tamaño tras dos poolings (28x28 -> 14x14 -> 7x7)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(c2 * 7 * 7, fc_hidden)\n",
    "        self.fc_out = nn.Linear(fc_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62709741",
   "metadata": {},
   "source": [
    "\n",
    "## Paso 2 — Construcción y experimentos de la CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimento CNN: {'model': 'cnn', 'c1': 16, 'c2': 32, 'fc_hidden': 128, 'opt': 'adam', 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eduar\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10 | Train Loss 0.2593 Acc 0.9245 | Val Loss 0.0926 Acc 0.9740\n",
      "Epoch 02/10 | Train Loss 0.0665 Acc 0.9796 | Val Loss 0.0577 Acc 0.9833\n",
      "Epoch 03/10 | Train Loss 0.0479 Acc 0.9851 | Val Loss 0.0510 Acc 0.9859\n",
      "Epoch 04/10 | Train Loss 0.0343 Acc 0.9895 | Val Loss 0.0518 Acc 0.9845\n",
      "Epoch 05/10 | Train Loss 0.0282 Acc 0.9911 | Val Loss 0.0492 Acc 0.9855\n",
      "Epoch 06/10 | Train Loss 0.0239 Acc 0.9924 | Val Loss 0.0461 Acc 0.9861\n",
      "Epoch 07/10 | Train Loss 0.0165 Acc 0.9950 | Val Loss 0.0401 Acc 0.9882\n",
      "Epoch 08/10 | Train Loss 0.0146 Acc 0.9952 | Val Loss 0.0411 Acc 0.9886\n",
      "Epoch 09/10 | Train Loss 0.0115 Acc 0.9962 | Val Loss 0.0447 Acc 0.9880\n",
      "Epoch 10/10 | Train Loss 0.0108 Acc 0.9964 | Val Loss 0.0438 Acc 0.9890\n",
      "\n",
      "Experimento CNN: {'model': 'cnn', 'c1': 32, 'c2': 64, 'fc_hidden': 256, 'opt': 'sgd', 'lr': 0.01}\n",
      "Epoch 01/10 | Train Loss 0.3268 Acc 0.9038 | Val Loss 0.1122 Acc 0.9638\n",
      "Epoch 02/10 | Train Loss 0.0689 Acc 0.9789 | Val Loss 0.0617 Acc 0.9816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def make_optimizer(name: str, params, lr: float):\n",
    "    name = name.lower()\n",
    "    if name == \"adam\":\n",
    "        return torch.optim.Adam(params, lr=lr)\n",
    "    elif name == \"sgd\":\n",
    "        return torch.optim.SGD(params, lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizador no reconocido: {name}\")\n",
    "\n",
    "# Lista de configuraciones de CNN a probar\n",
    "cnn_experiments = [\n",
    "    {\"model\": \"cnn\", \"c1\": 16, \"c2\": 32, \"fc_hidden\": 128, \"opt\": \"adam\", \"lr\": 1e-3},\n",
    "    {\"model\": \"cnn\", \"c1\": 32, \"c2\": 64, \"fc_hidden\": 256, \"opt\": \"sgd\",  \"lr\": 0.01},\n",
    "    {\"model\": \"cnn\", \"c1\": 8,  \"c2\": 16, \"fc_hidden\": 64,  \"opt\": \"adam\", \"lr\": 1e-3},\n",
    "]\n",
    "\n",
    "results: List[Dict[str, Any]] = []\n",
    "\n",
    "for cfg in cnn_experiments:\n",
    "    print(\"\\nExperimento CNN:\", cfg, )\n",
    "    model = SimpleCNN(c1=cfg[\"c1\"], c2=cfg[\"c2\"], fc_hidden=cfg[\"fc_hidden\"]).to(device)\n",
    "    opt = make_optimizer(cfg[\"opt\"], model.parameters(), lr=cfg[\"lr\"])\n",
    "\n",
    "    hist = train_model(model, train_loader, val_loader, opt, criterion, device, epochs=EPOCHS)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    params = count_params(model)\n",
    "\n",
    "    results.append({\n",
    "        \"name\": f\"CNN(c1={cfg['c1']},c2={cfg['c2']},fc={cfg['fc_hidden']},{cfg['opt']})\",\n",
    "        \"type\": \"CNN\",\n",
    "        \"val_acc\": val_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"params\": params,\n",
    "        \"model_obj\": model,  \n",
    "    })\n",
    "\n",
    "# Mostrar resultados ordenados por val_acc\n",
    "results_sorted = sorted(results, key=lambda d: d[\"val_acc\"], reverse=True)\n",
    "for r in results_sorted:\n",
    "    print(f\"{r['name']}: val_acc={r['val_acc']:.4f}, test_acc={r['test_acc']:.4f}, params={r['params']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce08dc6",
   "metadata": {},
   "source": [
    "\n",
    "### Línea base (laboratorio anterior)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes, activation_fn):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for in_dim, out_dim in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if out_dim != layer_sizes[-1]:\n",
    "                layers.append(activation_fn())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "BaselineAnterior = MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41538e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_cfg = {\"hidden1\": 256, \"hidden2\": 128, \"opt\": \"adam\", \"lr\": 1e-3}\n",
    "\n",
    "print(\"\\nEntrenando modelo anterior\")\n",
    "\n",
    "use_user_baseline = False\n",
    "baseline = None\n",
    "\n",
    "try:\n",
    "    BaselineAnterior  \n",
    "    try:\n",
    "        baseline = BaselineAnterior().to(device)\n",
    "        use_user_baseline = True\n",
    "        print(\"Usando BaseLine anterior\")\n",
    "    except TypeError as e:\n",
    "        print(\"La clase BaselineAnterior requiere argumentos:\", e)\n",
    "        print(\" Ajustar la instanciación aquí para pasar los parámetros adecuados.\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "if baseline is None:\n",
    "    print(\"Usando fallback MLPBaseline()\")\n",
    "    baseline = MLPBaseline(hidden1=baseline_cfg[\"hidden1\"], hidden2=baseline_cfg[\"hidden2\"]).to(device)\n",
    "\n",
    "opt_base = torch.optim.Adam(baseline.parameters(), lr=baseline_cfg[\"lr\"])\n",
    "\n",
    "_ = train_model(baseline, train_loader, val_loader, opt_base, criterion, device, epochs=EPOCHS)\n",
    "val_loss_b, val_acc_b = evaluate(baseline, val_loader, criterion, device)\n",
    "test_loss_b, test_acc_b = evaluate(baseline, test_loader, criterion, device)\n",
    "params_b = count_params(baseline)\n",
    "\n",
    "baseline_result = {\n",
    "    \"name\": (\"TuBaseline(BaselineAnterior)\" if use_user_baseline else f\"MLP(h1={baseline_cfg['hidden1']},h2={baseline_cfg['hidden2']})\"),\n",
    "    \"type\": \"BASELINE\",\n",
    "    \"val_acc\": val_acc_b,\n",
    "    \"test_acc\": test_acc_b,\n",
    "    \"params\": params_b,\n",
    "    \"model_obj\": baseline,\n",
    "}\n",
    "\n",
    "print(f\"Baseline -> val_acc={val_acc_b:.4f}, test_acc={test_acc_b:.4f}, params={params_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f261ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resumen tabular de resultados\n",
    "import pandas as pd\n",
    "\n",
    "all_results = results_sorted + [baseline_result]\n",
    "df = pd.DataFrame([\n",
    "    {\"Modelo\": r[\"name\"], \"Tipo\": r[\"type\"], \"Val_Acc\": r[\"val_acc\"], \"Test_Acc\": r[\"test_acc\"], \"Parámetros\": r[\"params\"],\n",
    "     \"Acc_por_MillónParams\": (r[\"test_acc\"] / (r[\"params\"]/1_000_000)) if r[\"params\"]>0 else float(\"nan\")}\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "# Orden por Test_Acc desc\n",
    "df = df.sort_values(by=\"Test_Acc\", ascending=False).reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bca46b",
   "metadata": {},
   "source": [
    "\n",
    "## Paso 3 — Benchmark: CNN vs Línea base\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdf73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tomamos la mejor CNN según validación\n",
    "best_cnn = next((r for r in results_sorted if r[\"type\"] == \"CNN\"), None)\n",
    "assert best_cnn is not None, \"No hay resultados de CNN.\"\n",
    "\n",
    "#  Matriz de confusión para la mejor CNN \n",
    "cm_cnn = confusion_matrix(best_cnn[\"model_obj\"], test_loader, num_classes=10, device=device)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm_cnn.numpy(), interpolation=\"nearest\")\n",
    "plt.title(\"Matriz de confusión – Mejor CNN\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Etiqueta real\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Matriz de confusión para la línea base \n",
    "cm_base = confusion_matrix(baseline_result[\"model_obj\"], test_loader, num_classes=10, device=device)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm_base.numpy(), interpolation=\"nearest\")\n",
    "plt.title(\"Matriz de confusión - Línea base (MLP)\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Etiqueta real\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumen textual automático\n",
    "def pretty_pct(x: float) -> str:\n",
    "    return f\"{100.0*x:.2f}%\"\n",
    "\n",
    "print(\"\\nResumen benchmark \")\n",
    "print(f\"Mejor CNN: {best_cnn['name']} | Val_Acc={pretty_pct(best_cnn['val_acc'])} | Test_Acc={pretty_pct(best_cnn['test_acc'])} | Params={best_cnn['params']:,}\")\n",
    "print(f\"Baseline:  {baseline_result['name']} | Val_Acc={pretty_pct(baseline_result['val_acc'])} | Test_Acc={pretty_pct(baseline_result['test_acc'])} | Params={baseline_result['params']:,}\")\n",
    "\n",
    "eff_cnn  = best_cnn[\"test_acc\"] / (best_cnn[\"params\"] / 1_000_000)\n",
    "eff_base = baseline_result[\"test_acc\"] / (baseline_result[\"params\"] / 1_000_000)\n",
    "print(f\"Eficiencia (Acc por millón de parámetros) -> CNN: {eff_cnn:.4f}  vs  Baseline: {eff_base:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
